{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai-key')"
      ],
      "metadata": {
        "id": "WXsrzCnsNOyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy\n",
        "!pip install text-to-number\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhkLlpiF82Wu",
        "outputId": "cac3021b-a3c9-4977-cbd0-d43523b72c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.25.2)\n",
            "Collecting text-to-number\n",
            "  Downloading text_to_number-0.1.2-py3-none-any.whl (4.8 kB)\n",
            "Installing collected packages: text-to-number\n",
            "Successfully installed text-to-number-0.1.2\n",
            "Collecting openai\n",
            "  Downloading openai-1.25.2-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# Initialize the OpenAI client with your API key\n",
        "OpenAI.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "def parse_output_to_dict(output):\n",
        "    # Initialize the dictionary\n",
        "    data_dict = {}\n",
        "\n",
        "    # Split the output into lines\n",
        "    lines = output.split('\\n')\n",
        "\n",
        "    # Extract and parse each line\n",
        "    for line in lines:\n",
        "        if line.startswith(\"Category:\"):\n",
        "            data_dict['Category'] = line.split(': ')[1]\n",
        "        elif line.startswith(\"Number of categories:\"):\n",
        "            data_dict['Number of categories'] = int(line.split(': ')[1])\n",
        "        elif line.startswith(\"Categories:\"):\n",
        "            data_dict['Categories'] = line.split(': ')[1].split(', ')\n",
        "        elif line.startswith(\"Type:\"):\n",
        "            data_dict['Type'] = line.split(': ')[1]\n",
        "        elif line.startswith(\"Number of types:\"):\n",
        "            data_dict['Number of types'] = int(line.split(': ')[1])\n",
        "        elif line.startswith(\"Types:\"):\n",
        "            data_dict['Types'] = line.split(': ')[1].split(', ')\n",
        "        elif line.startswith(\"Observed frequency:\"):\n",
        "          # Process observed frequencies to convert them into a list of lists of integers\n",
        "          frequencies = line.split(': ')[1].strip('[]').split('], [')\n",
        "          data_dict['Observed frequency'] = [list(map(int, freq.strip('[]').split(', '))) for freq in frequencies]\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "def get_output(question):\n",
        "    # prompt = f\"Given the question: '{question}', please categorize the question and provide the observed frequencies. For example, if the question is about a male buying a car, the category could be 'Male' and the type 'bought a car'. The observed frequencies could be '207, 811, 65, 984'.\"\n",
        "    prompt = f\"Given the question: '{question}', please categorize the question and provide the observed frequencies. For example, if the question is about a male buying a car, the category could be 'Male' and the type 'bought a car'. The observed frequencies could be '207, 811, 65, 984'.\\n\\nCategory: Gender\\nNumber of categories: 2\\nCategories: Male, Female\\nType: Purchase Decision\\nNumber of types: 2\\nTypes: Bought, Not Bought\\nObserved frequency: [[207, 811], [65, 984]]\"\n",
        "\n",
        "    # Make an API call to OpenAI using the appropriate method\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",  # Adjust model as necessary\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=150,\n",
        "        temperature=0.5\n",
        "    )\n",
        "\n",
        "    # Extracting and processing the response according to your structure\n",
        "    # output = response.choices[0].message['content'].strip()  # Adjusted access to 'content'\n",
        "    output = response.choices[0].message.content.strip()  # Adjusted access to 'content'\n",
        "    # print(f\"output: {output}\")\n",
        "    output=parse_output_to_dict(output)\n",
        "    # response_message = response.choices[0].message.content\n",
        "\n",
        "    return output\n",
        "\n",
        "# Example usage\n",
        "question = \"A researcher interviewed 2067 people and asked whether they were the primary decision makers in the household when buying a new car last year. Two hundred seven were men and had bought a new car last year. Sixty-five were women and had bought a new car last year. Eight hundred eleven of the responses were from men who did not buy a car last year. Nine hundred eighty-four were from women who did not buy a car last year. Use these data to determine whether gender is independent of being a major decision maker in purchasing a car last year. Let a = .05.\"\n",
        "output = get_output(question)\n",
        "print(output['Observed frequency'][0][0])\n",
        "isinstance(output['Observed frequency'][0][0],str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46vPtPe7mQbn",
        "outputId": "a4b432c4-f6ac-413a-cd2b-3b16e9a00f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "207\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import pandas as pd\n",
        "import re\n",
        "import scipy\n",
        "from scipy.stats import chi2\n",
        "import numpy as np\n",
        "from text_to_number import text_to_number as t2n"
      ],
      "metadata": {
        "id": "WjmIcllUK-dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "rR9dM3j3LhzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c23f9b98-61d0-49ca-df47-bec8b4495d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"A researcher interviewed 2067 people and asked whether they were the primary decision makers in the household when buying a new car last year. Two hundred seven were men and had bought a new car last year. Sixty-five were women and had bought a new car last year. Eight hundred eleven of the responses were from men who did not buy a car last year. Nine hundred eighty-four were from women who did not buy a car last year. Use these data to determine whether gender is independent of being a major decision maker in purchasing a car last year. Let a = .05.\""
      ],
      "metadata": {
        "id": "8OfWicy-yl1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)\n",
        "print(t2n(text))"
      ],
      "metadata": {
        "id": "Zjf89TOATbQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ef665c-3333-4865-ccff-36387ce3f164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A researcher interviewed 2067 people and asked whether they were the primary decision makers in the household when buying a new car last year. Two hundred seven were men and had bought a new car last year. Sixty-five were women and had bought a new car last year. Eight hundred eleven of the responses were from men who did not buy a car last year. Nine hundred eighty-four were from women who did not buy a car last year. Use these data to determine whether gender is independent of being a major decision maker in purchasing a car last year. Let a = .05.\n",
            "A researcher interviewed 2067 people and asked whether they were the primary decision makers in the household when buying a new car last year. 207 were men and had bought a new car last year. 65 were women and had bought a new car last year. 811 of the responses were from men who did not buy a car last year. 984 were from women who did not buy a car last year. Use these data to determine whether gender is independent of being a major decision maker in purchasing a car last year. Let a = .05.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDkFu2AMU5-O"
      },
      "outputs": [],
      "source": [
        "def isChi(text,fo,fe):\n",
        "  doc = nlp(text)\n",
        "  chi_squared_matcher = Matcher(nlp.vocab)\n",
        "  chi_squared_patterns = [\n",
        "    [{\"LOWER\": \"chi\"}, {\"LOWER\": \"squared\"}],\n",
        "    [{\"LOWER\": \"association\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}],\n",
        "    [{\"LOWER\": \"chisquare\"}],\n",
        "    [{\"LOWER\": \"observed\"}],\n",
        "    [{\"LOWER\": \"expected\"}],\n",
        "    [{\"LOWER\": \"poisson\"}],\n",
        "    [{\"LOWER\": \"fits\"}],\n",
        "    [{\"LOWER\": \"fit\"}],\n",
        "    [{\"LOWER\": \"independent\"}],\n",
        "    [{\"LOWER\": \"chi2\"}],\n",
        "    [{\"LOWER\": \"χ2\"}],\n",
        "    [{\"LOWER\": \"pearson's\"}, {\"LOWER\": \"chi-squared\"}],\n",
        "    [{\"LOWER\": \"pearson\"}, {\"LOWER\": \"chi-square\"}],\n",
        "    [{\"LOWER\": \"contingency\"}],\n",
        "    [{\"LOWER\": \"goodness-of-fit\"}],\n",
        "    [{\"LOWER\": \"independence\"}],\n",
        "    [{\"LOWER\": \"categorical\"}],\n",
        "    [{\"LOWER\": \"χ-square\"}],\n",
        "    [{\"LOWER\": \"chi\"}],\n",
        "    [{\"LOWER\": \"chi2-square\"}],\n",
        "    [{\"LOWER\": \"chi-sq\"}],\n",
        "    [{\"LOWER\": \"χ2-square\"}],\n",
        "    [{\"LOWER\": \"χ-square\"}, {\"LOWER\": \"analysis\"}],\n",
        "    [{\"LOWER\": \"pearson's\"}],\n",
        "    [{\"LOWER\": \"pearson\"}],\n",
        "    [{\"LOWER\": \"contingency\"}, {\"LOWER\": \"analysis\"}],\n",
        "    [{\"LOWER\": \"goodness\"}, {\"LOWER\": \"of\"}, {\"LOWER\": \"fit\"}, {\"LOWER\": \"analysis\"}],\n",
        "    [{\"LOWER\": \"independence\"}, {\"LOWER\": \"analysis\"}],\\\n",
        "    [{\"LOWER\": \"association\"}, {\"LOWER\": \"analysis\"}],\n",
        "    [{\"LOWER\": \"χ2\"}],\n",
        "    [{\"LOWER\": \"chi-squared\"}, {\"LOWER\": \"homogeneity\"}],\n",
        "    [{\"LOWER\": \"categorical\"}, {\"LOWER\": \"variable\"}, {\"LOWER\": \"analysis\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"p-value\"}],\n",
        "    [{\"LOWER\": \"chi-squared\"}, {\"LOWER\": \"significance\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"critical\"}, {\"LOWER\": \"region\"}],\n",
        "    [{\"LOWER\": \"categorical\"}, {\"LOWER\": \"relationship\"}],\n",
        "    [{\"LOWER\": \"chi-sq\"}],\n",
        "    [{\"LOWER\": \"χ2\"}, {\"LOWER\": \"goodness-of-fit\"}],\n",
        "    [{\"LOWER\": \"χ-square\"}, {\"LOWER\": \"of\"}, {\"LOWER\": \"independence\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"analysis\"}],\n",
        "    [{\"LOWER\": \"chi2\"}],\n",
        "    [{\"LOWER\": \"χ2\"}],\n",
        "    [{\"LOWER\": \"χ2\"}, {\"LOWER\": \"contingency\"}],\n",
        "    [{\"LOWER\": \"discrete\"}],\n",
        "    [{\"LOWER\": \"chi-squared\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"statistic\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"p-value\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"critical\"}, {\"LOWER\": \"value\"}],\n",
        "    [{\"LOWER\": \"χ-square\"}, {\"LOWER\": \"distribution\"}],\n",
        "    [{\"LOWER\": \"chi-sq\"}],\n",
        "    [{\"LOWER\": \"χ2\"}, {\"LOWER\": \"goodness-of-fit\"}],\n",
        "    [{\"LOWER\": \"χ-square\"}, {\"LOWER\": \"of\"}, {\"LOWER\": \"independence\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"analysis\"}],\n",
        "    [{\"LOWER\": \"chi2\"}],\n",
        "    [{\"LOWER\": \"χ2\"}],\n",
        "    [{\"LOWER\": \"χ2\"}, {\"LOWER\": \"contingency\"}],\n",
        "    [{\"LOWER\": \"discrete\"}],\n",
        "    [{\"LOWER\": \"chi-squared\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"statistic\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"p-value\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"critical\"}, {\"LOWER\": \"value\"}],\n",
        "    [{\"LOWER\": \"χ-square\"}, {\"LOWER\": \"distribution\"}],\n",
        "    [{\"LOWER\": \"χ2\"}],\n",
        "    [{\"LOWER\": \"chi-squared\"}, {\"LOWER\": \"for\"}, {\"LOWER\": \"independence\"}],\n",
        "    [{\"LOWER\": \"chi-squared\"}, {\"LOWER\": \"contingency\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"distribution\"}],\n",
        "    [{\"LOWER\": \"categorical\"}, {\"LOWER\": \"analysis\"}],\n",
        "    [{\"LOWER\": \"crosstab\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"goodness-of-fit\"}],\n",
        "    [{\"LOWER\": \"chi-squared\"}, {\"LOWER\": \"homogeneity\"}],\n",
        "    [{\"LOWER\": \"categorical\"}, {\"LOWER\": \"variable\"}, {\"LOWER\": \"analysis\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"p-value\"}],\n",
        "    [{\"LOWER\": \"chi-squared\"}, {\"LOWER\": \"significance\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"critical\"}, {\"LOWER\": \"region\"}],\n",
        "    [{\"LOWER\": \"categorical\"}, {\"LOWER\": \"relationship\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"significance\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"critical\"}, {\"LOWER\": \"values\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"degrees\"}, {\"LOWER\": \"freedom\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"independence\"}],\n",
        "    [{\"LOWER\": \"contingency\"}],\n",
        "    [{\"LOWER\": \"categorical\"}, {\"LOWER\": \"association\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"null\"}, {\"LOWER\": \"hypothesis\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"alternative\"}, {\"LOWER\": \"hypothesis\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"hypothesis\"}],\n",
        "    [{\"LOWER\": \"chi-squared\"}, {\"LOWER\": \"for\"}, {\"LOWER\": \"goodness-of-fit\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"contingency\"}],\n",
        "    [{\"LOWER\": \"categorical\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"statistical\"}],\n",
        "    [{\"LOWER\": \"pearson\"}, {\"LOWER\": \"chi-squared\"}, {\"LOWER\": \"statistic\"}],\n",
        "    [{\"LOWER\": \"contingency\"}, {\"LOWER\": \"independence\"}],\n",
        "    [{\"LOWER\": \"χ2-square\"}],\n",
        "    [{\"LOWER\": \"association\"}, {\"LOWER\": \"chi-square\"}],\n",
        "    [{\"LOWER\": \"categorical\"}, {\"LOWER\": \"independence\"}],\n",
        "    [{\"LOWER\": \"χ2\"}, {\"LOWER\": \"for\"}, {\"LOWER\": \"homogeneity\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"analysis\"}],\n",
        "    [{\"LOWER\": \"chi-squared\"}, {\"LOWER\": \"for\"}, {\"LOWER\": \"categorical\"}, {\"LOWER\": \"variables\"}],\n",
        "    [{\"LOWER\": \"χ-square\"}, {\"LOWER\": \"for\"}, {\"LOWER\": \"non-parametric\"}],\n",
        "    [{\"LOWER\": \"chi-square\"}, {\"LOWER\": \"for\"}, {\"LOWER\": \"observed\"}, {\"LOWER\": \"and\"}, {\"LOWER\": \"expected\"}],\n",
        "  ]\n",
        "\n",
        "  t_test_pattern = [{\"LOWER\": \"t-test\"}]\n",
        "  anova_pattern = [{\"LOWER\": \"anova\"}]\n",
        "  regression_pattern = [{\"LOWER\": \"regression\"}]\n",
        "  correlation_pattern = [{\"LOWER\": \"correlation\"}]\n",
        "  z_test_pattern = [{\"LOWER\": \"z-test\"}]\n",
        "  f_test_pattern = [{\"LOWER\": \"f-test\"}, {\"LOWER\": \"analysis\"}, {\"LOWER\": \"variance\"}]\n",
        "  wilcoxon_pattern = [{\"LOWER\": \"wilcoxon\"}, {\"LOWER\": \"rank-sum\"}]\n",
        "  mann_whitney_pattern = [{\"LOWER\": \"mann-whitney\"}, {\"LOWER\": \"u\"}]\n",
        "  kruskal_wallis_pattern = [{\"LOWER\": \"kruskal-wallis\"}]\n",
        "  mcnemar_pattern = [{\"LOWER\": \"mcnemar\"}]\n",
        "  logistic_regression_pattern = [{\"LOWER\": \"logistic\"}, {\"LOWER\": \"regression\"}]\n",
        "  cox_proportional_hazards_pattern = [{\"LOWER\": \"cox\"}, {\"LOWER\": \"proportional-hazards\"}]\n",
        "  mantel_haenszel_pattern = [{\"LOWER\": \"mantel-haenszel\"}]\n",
        "  kolmogorov_smirnov_pattern = [{\"LOWER\": \"kolmogorov-smirnov\"}]\n",
        "  chi_squared_matcher.add(\"ChiSquaredTest\", chi_squared_patterns)\n",
        "\n",
        "  chi_squared_matches = chi_squared_matcher(doc)\n",
        "\n",
        "  # Rule-based matching for other tests\n",
        "  other_tests_matcher = Matcher(nlp.vocab)\n",
        "  other_tests_matcher.add(\"OtherTests\", [\n",
        "      t_test_pattern, anova_pattern, regression_pattern, correlation_pattern,\n",
        "      z_test_pattern, f_test_pattern, wilcoxon_pattern, mann_whitney_pattern,\n",
        "      kruskal_wallis_pattern, mcnemar_pattern,\n",
        "      logistic_regression_pattern, cox_proportional_hazards_pattern,\n",
        "      mantel_haenszel_pattern, kolmogorov_smirnov_pattern\n",
        "  ])\n",
        "  other_tests_matches = other_tests_matcher(doc)\n",
        "  # Check if there are no matches for other tests and there are chi-squared test matches\n",
        "  return len(other_tests_matches) == 0 and len(chi_squared_matches) > 0\n",
        "\n",
        "def isGOF(text, fo, fe):\n",
        "    goodness_of_fit_patterns = [\n",
        "        r'chi[\\s-]*square[\\s-]*goodness[\\s-]*of[\\s-]*fit[\\s-]*test',\n",
        "        r'observed[\\s-]*frequencies',\n",
        "        r'expected[\\s-]*frequencies',\n",
        "        r'fit[\\s-]*test',\n",
        "        r'gof',\n",
        "        r'goodness[\\s-]*fit',\n",
        "        r'frequency[\\s-]*distribution[\\s-]*comparison',\n",
        "        r'distribution[\\s-]*fit',\n",
        "        r'fit[\\s-]*observed[\\s-]*expected',\n",
        "        r'chi[\\s-]*squared[\\s-]*fit',\n",
        "        r'chi[\\s-]*square[\\s-]*distribution[\\s-]*test',\n",
        "        r'frequency[\\s-]*fit',\n",
        "        r'fit[\\s-]*categorical[\\s-]*data',\n",
        "        r'categorical[\\s-]*distribution[\\s-]*test',\n",
        "        r'fit[\\s-]*test[\\s-]*for[\\s-]*distributions',\n",
        "        r'chi[\\s-]*2[\\s-]*goodness',\n",
        "        r'χ[\\s-]*squared[\\s-]*goodness',\n",
        "        r'chi[\\s-]*square[\\s-]*g[\\s-]*o[\\s-]*f',\n",
        "        r'chi[\\s-]*test[\\s-]*for[\\s-]*fit',\n",
        "        r'fitting[\\s-]*observed[\\s-]*to[\\s-]*expected',\n",
        "        r'conformity[\\s-]*of[\\s-]*observed[\\s-]*with[\\s-]*expected',\n",
        "        r'how[\\s-]*well[\\s-]*data[\\s-]*fits',\n",
        "        r'comparing[\\s-]*observed[\\s-]*to[\\s-]*expected',\n",
        "        r'uniform[\\s-]*distribution',\n",
        "        r'uniformly[\\s-]*distributed',\n",
        "        r'poisson[\\s-]*distributed',\n",
        "        r'normally[\\s-]*distributed',\n",
        "        r'normal[\\s-]*distribution'\n",
        "    ]\n",
        "\n",
        "    # Check for goodness-of-fit patterns\n",
        "    for pattern in goodness_of_fit_patterns:\n",
        "        if re.search(pattern, text, re.IGNORECASE):\n",
        "            return True\n",
        "\n",
        "    # If no patterns match, it's not a GOF type question\n",
        "    return False\n",
        "\n",
        "def isTFI(text,fo,fe):\n",
        "  independence_patterns = [\n",
        "    r'chi[\\s-]*square[\\s-]*test[\\s-]*of[\\s-]*independence',\n",
        "    r'contingency[\\s-]*table',\n",
        "    r'independence[\\s-]*test',\n",
        "    r'association[\\s-]*test',\n",
        "    r'relationship[\\s-]*between[\\s-]*categorical',\n",
        "    r'categorical[\\s-]*variables[\\s-]*association',\n",
        "    r'two[\\s-]*way[\\s-]*table',\n",
        "    r'two[\\s-]*way[\\s-]*anova',\n",
        "    r'cross[\\s-]*tabulation',\n",
        "    r'χ[\\s-]*square[\\s-]*for[\\s-]*independence',\n",
        "    r'χ[\\s-]*2[\\s-]*independence',\n",
        "    r'chi[\\s-]*2[\\s-]*association',\n",
        "    r'χ[\\s-]*squared[\\s-]*test[\\s-]*for[\\s-]*association',\n",
        "    r'chi[\\s-]*square[\\s-]*contingency',\n",
        "    r'categorical[\\s-]*data[\\s-]*association',\n",
        "    r'association[\\s-]*between[\\s-]*two[\\s-]*variables',\n",
        "    r'frequency[\\s-]*data[\\s-]*analysis',\n",
        "    r'χ[\\s-]*2[\\s-]*test[\\s-]*of[\\s-]*association',\n",
        "    # Added patterns to capture the use of \"independent\"\n",
        "    r'independent[\\s-]*of',\n",
        "    r'whether[\\s-]*.*[\\s-]*is[\\s-]*independent',\n",
        "    r'independent[\\s-]*variable',\n",
        "    r'dependent[\\s-]*and[\\s-]*independent',\n",
        "    r'variable[\\s-]*is[\\s-]*independent'\n",
        "  ]\n",
        "\n",
        "    # Check for test of independence patterns\n",
        "  for pattern in independence_patterns:\n",
        "    if re.search(pattern, text, re.IGNORECASE):\n",
        "      return True\n",
        "\n",
        "    # If no patterns match, it's not a TFI type question\n",
        "  return False\n",
        "\n",
        "def dataTableHasFe(fe):\n",
        "  return len(fe)>0\n",
        "\n",
        "def dataTableHasFo(fo):\n",
        "  if(fo==[]):\n",
        "    return False\n",
        "  if(fo==''):\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "def textHasFe(text):\n",
        "  return False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extractAlpha(text):\n",
        "    # Regular expressions to match different representations of alpha\n",
        "  regex_patterns = [\n",
        "    r\"α\\s*=\\s*(0?\\.\\d+|\\d+\\.\\d*)\",  # Explicit mention with symbol, allowing for \".05\" or \"0.05\"\n",
        "    r\"alpha\\s*=\\s*(0?\\.\\d+|\\d+\\.\\d*)\",  # Explicit mention with word\n",
        "    r\"at\\s+a\\s+significance\\s+level\\s+of\\s+(0?\\.\\d+|\\d+\\.\\d*)\",  # Descriptive representation\n",
        "    r\"set\\s+to\\s+(0?\\.\\d+|\\d+\\.\\d*)\",  # Descriptive representation with \"set to\"\n",
        "    r\"chosen\\s+a\\s+significance\\s+level\\s+of\\s+(0?\\.\\d+|\\d+\\.\\d*)\",  # Descriptive representation with \"chosen a significance level of\"\n",
        "    r\"with\\s+α\\s*=\\s*(0?\\.\\d+|\\d+\\.\\d*)\",  # Symbolic representation\n",
        "    r\"at\\s+a\\s+level\\s+of\\s+(0?\\.\\d+|\\d+\\.\\d*)\",  # Alternative descriptive representation\n",
        "    r\"p\\s*=\\s*(0?\\.\\d+|\\d+\\.\\d*)\",  # Representation using 'p'\n",
        "    r\"probability\\s*threshold\\s*was\\s*set\\s*to\\s*(0?\\.\\d+|\\d+\\.\\d*)\",  # More flexible representation using 'probability'\n",
        "    r\"a\\s*=\\s*(0?\\.\\d+|\\d+\\.\\d*)\",  # Explicit mention of 'a' with symbol\n",
        "    r\"s+\\s*a\\s*=\\s*(0?\\.\\d+|\\d+\\.\\d*)\",  # Corrected typo for 's+a' to allow spaces between 's' and 'a'\n",
        "    r\"s+\\s*alpha\\s*=\\s*(0?\\.\\d+|\\d+\\.\\d*)\"  # Corrected typo for 's+alpha' to allow spaces between 's' and 'alpha'\n",
        "  ]\n",
        "\n",
        "    # Search for alpha in the text using regular expressions\n",
        "  for pattern in regex_patterns:\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "    if match:\n",
        "      return float(match.group(1).replace(',', '.'))  # Replace comma with dot for consistency\n",
        "\n",
        "    # If alpha is not found, return default value 0.05\n",
        "  return 0.05"
      ],
      "metadata": {
        "id": "0RVhwRr5XovI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extractFo(text,fo):\n",
        "  if dataTableHasFo(fo):\n",
        "    return fo\n",
        "\n",
        "  # extract from fo\n",
        "  return extractFoFromText(text)\n",
        "\n",
        "def extractFoFromText(text):\n",
        "  # Pattern to find numeric values (both whole numbers and decimals)\n",
        "  number_pattern = r'\\b\\d+(?:\\.\\d+)?\\b'  # Use non-capturing group for decimal part\n",
        "\n",
        "  # Assuming extractAlpha has already been called and its value stored\n",
        "  alpha_value = extractAlpha(text)  # Call illustrative; in practice, avoid redundant calls\n",
        "\n",
        "  # Find all numbers in the text\n",
        "  all_numbers_matches = re.findall(number_pattern, text)\n",
        "\n",
        "  # Safely convert non-empty matches to floats\n",
        "  all_numbers_float = [float(match) for match in all_numbers_matches if match]\n",
        "\n",
        "  # Filter out the alpha value if it appears as a number in the text\n",
        "  observed_frequencies = [num for num in all_numbers_float if num != alpha_value]\n",
        "\n",
        "  return observed_frequencies\n",
        "\n",
        "def extractFe(text,fo,fe):\n",
        "  print(fe)\n",
        "  if (fe=='') or (fe==[]):\n",
        "    return extractFeFromText(text,fo)\n",
        "\n",
        "  if dataTableHasFe(fe):\n",
        "    # extract From DataTable\n",
        "    return fe\n",
        "\n",
        "  return extractFeFromText(text,fo)\n",
        "\n",
        "def extractFeFromText(text,fo):\n",
        "  # switch case for different\n",
        "  if(textHasFe(text)): #false for now\n",
        "    # nlp\n",
        "    return 110\n",
        "\n",
        "  switch = identifyDistribution(text)\n",
        "  fe=getFeFromFo(switch,fo)\n",
        "  return fe\n",
        "\n",
        "def getFeFromFo(distribution, fo):\n",
        "    # Convert non-integer elements to integers or handle them appropriately\n",
        "    fo = [int(item) if isinstance(item, (int, float)) else item[1] for item in fo]\n",
        "\n",
        "    if distribution == 'uniform':\n",
        "        total = sum(fo)\n",
        "        n_categories = len(fo)\n",
        "        return [total / n_categories for _ in fo]\n",
        "    elif distribution == 'poisson':\n",
        "        res = poisson(fo)\n",
        "        return res\n",
        "    elif distribution == 'binomial':\n",
        "        # Placeholder: Implement based on n (trials) and p (success probability)\n",
        "        return []\n",
        "    elif distribution == 'normal':\n",
        "        # Placeholder: Implement based on μ (mean) and σ (std dev)\n",
        "        res=normal(fo)\n",
        "        return res\n",
        "    else:\n",
        "        #just execute uniform\n",
        "        total = sum(fo)\n",
        "        n_categories = len(fo)\n",
        "        return [total / n_categories for _ in fo]\n",
        "\n",
        "def identifyDistribution(text):\n",
        "    if 'uniform' in text.lower() or 'uniformly' in text.lower():\n",
        "        print('uniform')\n",
        "        return 'uniform'\n",
        "    elif 'poisson' in text.lower():\n",
        "        print('poisson')\n",
        "        return 'poisson'\n",
        "    elif 'binomial' in text.lower():\n",
        "        print('binomial')\n",
        "        return 'binomial'\n",
        "    elif 'normal' in text.lower():\n",
        "        print('normal')\n",
        "        return 'normal'\n",
        "    else:\n",
        "        print('uniform because no other')\n",
        "        return None\n",
        "\n",
        "from math import exp, factorial\n",
        "\n",
        "from scipy.stats import norm\n",
        "import numpy as np\n",
        "\n",
        "def normal(fo):\n",
        "    # Estimate mean (μ) and standard deviation (σ) from fo\n",
        "    total_observations = sum(freq for _, freq in fo)\n",
        "    mean = sum(midpoint * freq for midpoint, freq in fo) / total_observations\n",
        "    variance = sum(freq * ((midpoint - mean) ** 2) for midpoint, freq in fo) / total_observations\n",
        "    std_dev = np.sqrt(variance)\n",
        "\n",
        "    # Calculate expected frequencies\n",
        "    fe = []\n",
        "    for i, (midpoint, _) in enumerate(fo):\n",
        "        if i == 0:\n",
        "            # For the first category, assume lower bound is far enough to approximate -infinity\n",
        "            lower_bound = -np.inf\n",
        "        else:\n",
        "            # Use the midpoint between this category and the previous one\n",
        "            lower_bound = (fo[i][0] + fo[i-1][0]) / 2\n",
        "\n",
        "        if i == len(fo) - 1:\n",
        "            # For the last category, assume upper bound is far enough to approximate infinity\n",
        "            upper_bound = np.inf\n",
        "        else:\n",
        "            # Use the midpoint between this category and the next one\n",
        "            upper_bound = (fo[i][0] + fo[i+1][0]) / 2\n",
        "\n",
        "        # Calculate cumulative probability for lower and upper bounds\n",
        "        lower_cdf = norm.cdf(lower_bound, mean, std_dev)\n",
        "        upper_cdf = norm.cdf(upper_bound, mean, std_dev)\n",
        "\n",
        "        # Expected frequency for this category\n",
        "        expected_frequency = (upper_cdf - lower_cdf) * total_observations\n",
        "        fe.append(expected_frequency)\n",
        "\n",
        "    return fe\n",
        "\n",
        "# Example usage\n",
        "# Suppose fo is given as [(midpoint, frequency), ...]\n",
        "fo = [(15, 6), (25, 14), (35, 29), (45, 38), (55, 25), (65, 10), (75, 7)]\n",
        "fe_normal = normal(fo)\n",
        "print(\"Expected frequencies (Normal):\", fe_normal)\n",
        "\n",
        "def preprocess_fo(fo):\n",
        "    # Check if the input is already in the expected format (list of tuples)\n",
        "    if all(isinstance(item, tuple) for item in fo):\n",
        "        return fo\n",
        "\n",
        "    # If not, assume it's a list of frequencies starting from 0 arrivals\n",
        "    return [(i, freq) for i, freq in enumerate(fo)]\n",
        "\n",
        "def poisson(fo):\n",
        "    # Preprocess input to ensure it's in the correct format\n",
        "    fo = preprocess_fo(fo)\n",
        "\n",
        "    # Calculate total number of observations\n",
        "    N = sum(frequency for _, frequency in fo)\n",
        "\n",
        "    # Calculate λ (lambda): average rate of occurrences\n",
        "    lambda_ = sum(x * frequency for x, frequency in fo) / N\n",
        "    print(\"λ =\",lambda_)\n",
        "\n",
        "    # Calculate expected probabilities and frequencies for each x using the Poisson formula\n",
        "    fe = [(x, ((lambda_ ** x) * exp(-lambda_) / factorial(x)) * N) for x, _ in fo]\n",
        "\n",
        "    # Return only the expected frequencies, discarding x\n",
        "    expected_frequencies = [expected for _, expected in fe]\n",
        "\n",
        "    return expected_frequencies\n",
        "\n",
        "# # Example usage:\n",
        "# fo = [207, 65, 811, 984]  # Assuming this represents frequencies of 0, 1, 2, 3... arrivals\n",
        "# expected_frequencies = poisson(fo)\n",
        "# print(\"Expected frequencies:\", expected_frequencies)\n",
        "\n",
        "# # Example usage\n",
        "# # fo should be a list of tuples (x, observed frequency of x)\n",
        "# # For example: [(0, 18), (1, 36), (2, 25), (3, 12), (4, 9)]\n",
        "# fo = [(0, 18), (1, 36), (2, 25), (3, 12), (4, 9)]\n",
        "# expected_frequencies = poisson(fo)\n",
        "# print(expected_frequencies)\n",
        "\n",
        "def extractN(text,dataTable):\n",
        "  if dataTable=='':\n",
        "    return extractNFromText(text)\n",
        "\n",
        "  # get from data table counting fo\n",
        "  return 10\n",
        "\n",
        "def extractNFromText(text):\n",
        "  # Nlp\n",
        "  return 10\n",
        "\n",
        "# Function to solve Chi-square goodness of fit test\n",
        "import numpy as np\n",
        "from scipy.stats import chi2\n",
        "\n",
        "def solveGOF(fo, fe, n, alpha):\n",
        "    # Define null and alternate hypothesis\n",
        "    null_hypothesis = \"Null Hypothesis (H0): Observed frequencies are consistent with the expected frequencies.\"\n",
        "    alternate_hypothesis = \"Alternate Hypothesis (H1): Observed frequencies differ significantly from expected frequencies.\"\n",
        "\n",
        "    # Convert lists to numpy arrays for element-wise operations\n",
        "    fo = np.array(fo)\n",
        "    fe = np.array(fe)\n",
        "\n",
        "    # Calculate Chi-square test statistic\n",
        "    chi_square = np.sum((fo - fe)**2 / fe)\n",
        "\n",
        "    # Determine degrees of freedom\n",
        "    df = n - 1  # Adjusted based on the input parameter 'n', assuming it represents the number of categories\n",
        "\n",
        "    # Calculate p-value\n",
        "    p_value = 1 - chi2.cdf(chi_square, df)\n",
        "\n",
        "    # Determine critical value\n",
        "    critical_value = chi2.ppf(1 - alpha, df)\n",
        "\n",
        "    # Output\n",
        "    print(\"1) \")\n",
        "    print(null_hypothesis)\n",
        "    print(alternate_hypothesis)\n",
        "    print(\"2) Chi Square test is appropriate\")\n",
        "    print(\"3) Alpha Value:\", alpha)\n",
        "    print(\"4) Critical Value:\", critical_value)\n",
        "    print(\"5) Chi-square value:\", chi_square)\n",
        "    print(\"6) Final Output:\")\n",
        "\n",
        "    # Compare p-value with significance level\n",
        "    if p_value < alpha:\n",
        "        print(\"Reject null hypothesis. Observed frequencies differ significantly from expected frequencies.\")\n",
        "    else:\n",
        "        print(\"Fail to reject null hypothesis. No significant difference between observed and expected frequencies.\")\n",
        "\n",
        "# # Example data\n",
        "# fo = np.array([60, 90, 75])  # Observed frequencies\n",
        "# fe = np.array([70, 80, 75])  # Expected frequencies\n",
        "# n = np.sum(fo)  # Total number of observations\n",
        "# alpha = 0.05  # Significance level\n",
        "\n",
        "# # Solve Chi-square goodness of fit test\n",
        "# solveGOF(fo, fe, n, alpha)\n"
      ],
      "metadata": {
        "id": "_Z1FRwTq9auP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f948a9f3-6095-4552-95ce-c27fcbb8addd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected frequencies (Normal): [5.856349356809644, 14.757003637426454, 28.70552877332967, 35.06434698118453, 26.901667542358773, 12.959840939034082, 4.755262769856849]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# New Functions for Test for Independence (TFI)\n",
        "def extractFoTFI(text,fo):\n",
        "    # Placeholder for TFI-specific observed frequency extraction\n",
        "    if fo==[]:\n",
        "      fo=get_output(text)['Observed frequency']\n",
        "    return fo\n",
        "\n",
        "def extractFeTFI(text, fo, fe):\n",
        "    # Total number of observations\n",
        "    n = sum(sum(row) for row in fo)\n",
        "\n",
        "    # Calculate row totals\n",
        "    row_totals = [sum(row) for row in fo]\n",
        "\n",
        "    # Calculate column totals\n",
        "    col_totals = [sum(fo[row][col] for row in range(len(fo))) for col in range(len(fo[0]))]\n",
        "\n",
        "    # Calculate expected frequencies for each cell\n",
        "    fe = [[(row_total * col_total) / n for col_total in col_totals] for row_total in row_totals]\n",
        "\n",
        "    return fe\n",
        "\n",
        "def extractNTFI(text,fo,fe):\n",
        "    # Placeholder for TFI-specific total count extraction\n",
        "    return 20\n",
        "\n",
        "def solveTFI(fo, fe, df, alpha):\n",
        "    print(\"Starting Test for Independence (TFI) analysis...\")\n",
        "\n",
        "    # Convert lists to numpy arrays for vectorized operations\n",
        "    fo = np.array(fo)\n",
        "    fe = np.array(fe)\n",
        "\n",
        "    # Step 1: Calculate the Chi-Square statistic\n",
        "    chi2_stat = np.sum((fo - fe) ** 2 / fe)\n",
        "    print(f\"\\nStep 1: Chi-Square Statistic Calculation\")\n",
        "    print(f\"Calculated Chi-Square Statistic: {chi2_stat:.4f}\")\n",
        "\n",
        "    # Step 2: Calculate the p-value\n",
        "    p_value = 1 - scipy.stats.chi2.cdf(chi2_stat, df)\n",
        "    print(f\"\\nStep 2: P-value Calculation\")\n",
        "    print(f\"Calculated P-value: {p_value:.4g}\")  # Using scientific notation if needed\n",
        "\n",
        "    # Step 3: Calculate the critical value\n",
        "    critical_value = scipy.stats.chi2.ppf(1 - alpha, df)\n",
        "    print(f\"\\nStep 3: Critical Value Calculation\")\n",
        "    print(f\"Calculated Critical Value: {critical_value:.4f}\")\n",
        "\n",
        "    # Step 4: Determine significance based on chi-square statistic and critical value\n",
        "    is_significant_chi2 = chi2_stat > critical_value\n",
        "    result_msg_chi2 = \"significant\" if is_significant_chi2 else \"not significant\"\n",
        "    print(f\"\\nStep 4: Significance Testing Based on Chi-Square Statistic\")\n",
        "    print(f\"Using alpha level of {alpha}, the chi-square statistic result is determined to be {result_msg_chi2}.\")\n",
        "\n",
        "    # Step 5: Determine significance based on p-value\n",
        "    is_significant_p = p_value < alpha\n",
        "    result_msg_p = \"significant\" if is_significant_p else \"not significant\"\n",
        "    print(f\"\\nStep 5: Significance Testing Based on P-value\")\n",
        "    print(f\"Using alpha level of {alpha}, the p-value result is determined to be {result_msg_p}.\")\n",
        "\n",
        "    # Final result\n",
        "    print(f\"\\nFinal Result: The association between the variables is {result_msg_p} based on p-value and {result_msg_chi2} based on chi-square statistic.\")\n",
        "    print(f\"Chi-Square Statistic: {chi2_stat:.4f}, P-value: {p_value:.4g}, Critical Value: {critical_value:.4f}, Significance Level (Alpha): {alpha}\")\n",
        "\n",
        "    return chi2_stat, p_value, critical_value, is_significant_p, is_significant_chi2\n",
        "\n",
        "\n",
        "def extractDfTfi(fo):\n",
        "    # Validate fo to ensure it's a list of lists\n",
        "    if not isinstance(fo, list) or not all(isinstance(row, list) for row in fo):\n",
        "        raise ValueError(\"fo must be a two-dimensional list.\")\n",
        "\n",
        "    r = len(fo)\n",
        "    c = len(fo[0]) if r > 0 else 0\n",
        "    df = (r - 1) * (c - 1)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "b6IgjBSdQ9rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessfo(fo):\n",
        "    # Check if 'fo' is a list of tuples\n",
        "    if all(isinstance(item, tuple) and len(item) == 2 for item in fo):\n",
        "        # Extract and return the second element from each tuple\n",
        "        return [frequency for _, frequency in fo]\n",
        "    else:\n",
        "        # If 'fo' is not a list of tuples, return it unchanged\n",
        "        return fo\n",
        "\n",
        "def preprocessfe(fe, fo):\n",
        "    if not isinstance(fe, list):\n",
        "        fe = [fe]\n",
        "\n",
        "    sum_fo = sum(fo)\n",
        "    sum_fe = sum(fe)\n",
        "    if 99 <= sum_fe <= 101:\n",
        "        multiplier = sum_fo / sum_fe\n",
        "        new_fe = [round(frequency * multiplier) for frequency in fe]\n",
        "        return new_fe\n",
        "    else:\n",
        "        return fe\n",
        "\n",
        "# # Example usage\n",
        "# fo_tuples = [(1, 10), (2, 20), (3, 30)]\n",
        "# fo_single = [10, 20, 30]\n",
        "\n",
        "# # Should return [10, 20, 30]\n",
        "# print(preprocessfo(fo_tuples))\n",
        "\n",
        "# # Should also return [10, 20, 30] (unchanged)\n",
        "# print(preprocessfo(fo_single))\n",
        "\n",
        "def main(text, fo,fe):\n",
        "    text=t2n(text)\n",
        "    if not isChi(text, fo,fe):\n",
        "        print(\"Not chi-square question\")\n",
        "        return\n",
        "\n",
        "    alpha = extractAlpha(text)  # Common to both tests\n",
        "\n",
        "    if isGOF(text,fo,fe):\n",
        "        fo = extractFo(text,fo)\n",
        "        fe = extractFe(text,fo,fe)\n",
        "        fo = preprocessfo(fo)\n",
        "        fe = preprocessfe(fe, fo)\n",
        "        n = extractN(text,fo)\n",
        "        print(\"fo\",fo)\n",
        "        print(\"fe\",fe)\n",
        "        print(\"n\",n)\n",
        "        result_gof = solveGOF(fo, fe, n, alpha)\n",
        "\n",
        "    if isTFI(text,fo,fe):\n",
        "        fo_tfi = extractFoTFI(text,fo)\n",
        "        fe_tfi = extractFeTFI(text,fo_tfi,fe)\n",
        "        n_tfi = extractNTFI(text,fo_tfi,fe)\n",
        "        df_tfi = extractDfTfi(fo_tfi)\n",
        "        print(\"fo\",fo_tfi)\n",
        "        print(\"fe\",fe_tfi)\n",
        "        print(\"n\",n_tfi)\n",
        "        print(\"df\",df_tfi)\n",
        "        result_tfi = solveTFI(fo_tfi, fe_tfi, df_tfi, alpha)\n",
        "\n",
        "        print(f\"Test for Independence Result: {result_tfi}\")\n"
      ],
      "metadata": {
        "id": "qtrKu15c9If8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "  text='''\n",
        "Use the following contingency table and the chi-square test of independence to determine whether social class is independent of number of children in a family. Let a = .05.\n",
        "  '''\n",
        "  fo=[[7,18,6,9],[38,23,34,97],[58,47,31,30]]\n",
        "  fe=[]\n",
        "  typ=\"\"\n",
        "  alpha=0.01\n",
        "  print(isChi(text,fo,fe),\"ischi\")\n",
        "  print(isGOF(text,fo,fe),\"GOF\")\n",
        "  print(isTFI(text,fo,fe),\"TFI\")\n",
        "  main(text,fo,fe)\n"
      ],
      "metadata": {
        "id": "zux5olIV7EFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "863f6f71-9b57-4202-9242-8db5b227218c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True ischi\n",
            "False GOF\n",
            "True TFI\n",
            "fo [[7, 18, 6, 9], [38, 23, 34, 97], [58, 47, 31, 30]]\n",
            "fe [[10.35175879396985, 8.844221105527637, 7.135678391959799, 13.668341708542714], [49.688442211055275, 42.45226130653266, 34.25125628140704, 65.60804020100502], [42.959798994974875, 36.7035175879397, 29.613065326633166, 56.72361809045226]]\n",
            "n 20\n",
            "df 6\n",
            "Starting Test for Independence (TFI) analysis...\n",
            "\n",
            "Step 1: Chi-Square Statistic Calculation\n",
            "Calculated Chi-Square Statistic: 59.8328\n",
            "\n",
            "Step 2: P-value Calculation\n",
            "Calculated P-value: 4.867e-11\n",
            "\n",
            "Step 3: Critical Value Calculation\n",
            "Calculated Critical Value: 12.5916\n",
            "\n",
            "Step 4: Significance Testing Based on Chi-Square Statistic\n",
            "Using alpha level of 0.05, the chi-square statistic result is determined to be significant.\n",
            "\n",
            "Step 5: Significance Testing Based on P-value\n",
            "Using alpha level of 0.05, the p-value result is determined to be significant.\n",
            "\n",
            "Final Result: The association between the variables is significant based on p-value and significant based on chi-square statistic.\n",
            "Chi-Square Statistic: 59.8328, P-value: 4.867e-11, Critical Value: 12.5916, Significance Level (Alpha): 0.05\n",
            "Test for Independence Result: (59.832819921011, 4.867106717654224e-11, 12.591587243743977, True, True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-8i_Da-L0Mh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}